

# **A Strategic Analysis of Data Assets for an AI-Powered Google Interview Simulation Platform**

## **Part I: Foundational Data for Technical Assessments**

The development of an artificial intelligence capable of simulating Google's rigorous technical interviews necessitates a data foundation of unparalleled breadth and quality. This initial part of the report focuses on the acquisition and strategic analysis of data assets required to model the two pillars of Google's technical evaluation: algorithmic problem-solving and the production of high-quality, maintainable code. The data landscape for these domains is bifurcated. The domain of code analysis is characterized by structured, high-quality academic datasets that provide a robust foundation for building evaluation models. In contrast, the domain of interview problems is dominated by proprietary, commercially-driven platforms, requiring a more complex and risk-managed acquisition strategy involving unofficial APIs, web scraping, and static dataset analysis. This section provides a detailed roadmap for navigating both terrains to construct a comprehensive and defensible data corpus.

### **Section 1: Algorithmic Problem and Solution Corpora**

The central objective of this section is to architect a comprehensive and continuously updated corpus of Google-style coding challenges. This is not merely a collection of problems but a structured repository containing detailed problem statements, rich metadata (difficulty, topic tags, company associations), a multiplicity of solutions in various programming languages, and a complete set of test cases for validation. This corpus will serve as the foundational asset for the AI's core functionalities, including problem generation, automated solution evaluation, and the delivery of targeted, constructive feedback to candidates.

#### **1.1 The LeetCode Ecosystem: Tapping the De Facto Standard**

LeetCode has established itself as the most critical single source of data for this project. Its prominence stems directly from its use of "company tags," which provide a highly relevant, pre-curated list of problems known to have been used in Google interviews.1 This feature allows for the construction of a training set with a high degree of fidelity to the actual interview experience. However, this high value is matched by significant access challenges, as LeetCode does not provide a public, official API for the bulk extraction of its problem and solution data. This commercial reality shapes the entire data acquisition strategy, necessitating a multi-faceted approach that balances immediacy, data freshness, and risk.

The absence of an official API is not an oversight but a core component of LeetCode's business model. It creates a "data moat" that protects the value of its premium subscription, which grants users access to these coveted company-tagged question lists. For the simulation platform to succeed, it cannot rely on a single, fragile access method. It must develop a resilient and legally defensible data acquisition infrastructure. This involves a strategic progression:

1. The initial value proposition of LeetCode is its curated, Google-tagged problem list.1  
2. There is no officially sanctioned method for bulk-downloading this premium, high-relevance data.  
3. The developer community has responded by creating unofficial APIs 2 and web scrapers 4, a clear market signal of the data's high value and scarcity.  
4. These third-party methods are inherently fragile, as they depend on reverse-engineered private APIs that LeetCode can alter without notice. They also operate in a legally ambiguous space, with LeetCode's Terms of Service prohibiting the unauthorized publication of its content.6  
5. Consequently, a strategy reliant solely on these methods would introduce a critical single point of failure and significant legal risk. A more robust strategy involves a phased approach: utilizing static, pre-scraped datasets for initial model development (Phase 1); building a resilient, ethical scraping infrastructure for continuous updates (Phase 2); and exploring user-permissioned data access (e.g., OAuth integration for users to connect their own accounts) as a long-term, compliant solution (Phase 3). This transforms the tactical problem of "how to get the data" into a strategic plan for building a diversified and defensible data acquisition pipeline.

**Unofficial APIs:** The high demand for programmatic access to LeetCode has led to the development of multiple unofficial APIs, which serve as a viable, albeit fragile, access vector.2 Projects such as the FastAPI-powered API detailed in 2 and the comprehensive

alfa-leetcode-api on GitHub 3 offer crucial functionalities. These APIs provide endpoints to retrieve lists of problems and, most importantly, allow for filtering by tags. This filtering capability is essential for programmatically isolating the Google-specific questions. The data is typically returned in a structured JSON format, making it readily ingestible. The primary risk associated with these APIs is their dependence on LeetCode's internal GraphQL API. Any change to LeetCode's private infrastructure could break these tools without warning, necessitating constant monitoring and maintenance.

**Static Datasets on Kaggle:** For immediate prototyping and initial model training, several pre-scraped LeetCode datasets are available on the Kaggle platform.7 These datasets are invaluable for accelerating the early stages of development. For example, the datasets described in 8 and 9 are provided as CSV files with a rich schema, including columns such as

Title, Description, Difficulty, is\_premium, Companies, and Related\_Topics. This structured, labeled data allows for the immediate development of models for problem classification, difficulty prediction, and topic-based recommendation. The significant limitation of these datasets is their static nature. The dataset in 9, for instance, was last updated in April 2021\. While useful for building foundational models, it cannot be the basis for a production system that must stay current with the latest interview trends and newly added problems.

**Ethical Scraping Strategy:** To ensure data freshness and completeness beyond what static datasets offer, a direct, in-house scraping strategy is a necessary component of the data pipeline. Several open-source Python-based tools on GitHub demonstrate the technical feasibility of this approach, typically using libraries like Selenium to automate browser interactions.4 These scripts can log into a user account to access premium features and scrape solved problems, or they can interact with LeetCode's internal API to fetch problem lists and details. The implementation of such a scraper must be executed with extreme care, adhering to ethical best practices. This includes respecting the

robots.txt file, implementing conservative rate limiting and exponential backoff to avoid overloading LeetCode's servers, and navigating the complex legal and ethical landscape of web scraping.13 As noted in a user discussion, LeetCode's Terms of Service prohibit the publishing of its content, but scraping for the purpose of internal model training may occupy a legal gray area that requires careful consideration and consultation with legal counsel.6

#### **1.2 Competitive Programming Platforms: Diversifying the Problem Corpus**

While LeetCode provides the most targeted data, diversifying the problem corpus with data from other major competitive programming platforms is crucial for building robust and generalizable AI models. These platforms offer vast repositories of high-quality algorithmic problems that are stylistically and structurally similar to those encountered in Google interviews. However, their data accessibility models vary significantly, which dictates a tailored acquisition strategy for each source.

**Codeforces:** This platform stands out due to its developer-friendly approach, offering a well-documented, public-facing API that is ideal for bulk data collection.18 The API provides endpoints to retrieve comprehensive data in JSON format, including problem sets, user submissions, and rich metadata such as problem tags (e.g., "dynamic programming," "graphs"). The academic paper presented in 18 provides a detailed methodology for using this API in conjunction with scraping to build a complete dataset, including problem specifications, test cases, and accepted solutions in multiple languages. The existence of large, pre-compiled datasets derived from Codeforces, such as the one on ModelScope containing over 10,000 unique problems and validated test cases, further confirms its status as a high-value, highly accessible data source.22 While the provided materials do not contain explicit API terms of service, the public documentation and widespread use suggest a permissive environment for data access.23

**HackerRank:** In contrast to Codeforces, HackerRank's primary API is the "HackerRank for Work" API, a commercial, enterprise-grade tool designed for corporate clients to create and manage their own hiring assessments.33 This REST-based API is powerful, allowing for the management of tests, candidates, and even individual questions. However, accessing its vast library of existing interview problems would almost certainly require a formal partnership and a significant financial investment, as its terms of service are geared toward enterprise use.33 For immediate data acquisition, a more practical approach is to use HackerRank's publicly available "Interview Preparation Kit".41 This kit provides a well-structured, curated list of classic interview problems, categorized by fundamental topics such as "Arrays," "Graphs," and "Dynamic Programming." This list can serve as a high-quality manifest for a targeted web scraping effort to collect the problem statements and public solutions.

**AtCoder, CodeChef, and TopCoder:** These platforms represent a middle ground in terms of data accessibility. They host valuable problem archives but generally lack formal, public-facing APIs for bulk data extraction. The data acquisition strategy for these sources will rely heavily on community-developed tools and direct scraping. For AtCoder, an unofficial NPM package 42 and some Python API documentation 43 indicate that the community has successfully reverse-engineered access methods. Similarly, CodeChef has several unofficial, scraper-based APIs available on GitHub 44, though its official website does not host a developer portal.46 TopCoder provides access to its extensive "Match Archive" and "Problem Archive" through its website, but it does not offer a clear API for programmatic, bulk downloads.48 Therefore, data acquisition from these platforms will require dedicated engineering effort to build and maintain custom scrapers.

A comparative analysis of these platforms is essential for allocating engineering resources effectively. It allows for the prioritization of targets, such as assigning junior engineers to work with the well-documented Codeforces API while dedicating more senior resources to the complex and legally sensitive task of developing a resilient LeetCode scraper. This strategic overview also informs business development efforts, clearly identifying platforms like HackerRank where a partnership is the only viable path to data access.

**Table 1: Comparative Analysis of Competitive Programming Platform APIs**

| Platform | API Availability | Key Data Points Accessible | Data Format | Rate Limits/Access Policy | Acquisition Priority |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **LeetCode** | Unofficial / Scraping Required | Problems, Solutions, Company Tags, Topic Tags, Difficulty | JSON, HTML | Undocumented, High Risk | **High** |
| **Codeforces** | Official (Public) | Problems, Solutions (Submissions), Test Cases, Topic Tags, Difficulty | JSON | Publicly Documented | **High** |
| **HackerRank** | Official (Commercial/Partner) | Problems, Solutions, Test Cases, Topic Tags, Difficulty | JSON | Enterprise Agreement | **Medium** (Partnership Track) |
| **AtCoder** | Unofficial / Scraping Required | Problems, Solutions (Submissions), Contest Data | JSON, HTML | Undocumented | **Medium** |
| **CodeChef** | Unofficial / Scraping Required | Problems, Solutions (Submissions), Contest Data | JSON, HTML | Undocumented | **Medium** |
| **TopCoder** | None (Scraping Required) | Problems, Match Editorials, Solutions (via Archive) | HTML | Undocumented | **Low** |

#### **1.3 Code Quality and Complexity Benchmarks: Building the Evaluator's Brain**

To move beyond simply checking for correctness, the AI must be able to evaluate the *quality* of a candidate's code. This requires training it on datasets that embody the principles of good software engineering. Fortunately, this domain is rich with high-quality, open-source academic datasets, providing a low-risk, high-reward path to building a sophisticated and defensible code evaluation engine.

The data available for this task enables a powerful, two-pronged architectural strategy for the code evaluation AI. The first component can be a "classic" machine learning model or a fine-tuned transformer, trained on the wealth of structured, labeled datasets. This component would be responsible for producing objective, verifiable scores for metrics like time complexity, using the CodeComplex dataset 51, and for flagging known anti-patterns and potential bugs, using the various bug detection corpora.52 The second component would be a large language model (LLM) operating as a qualitative evaluator. This LLM would not be trained on a massive labeled dataset but would instead be prompted with a detailed, synthesized rubric derived from Google's own public code review guidelines 53 and general "clean code" principles.54 This hybrid approach leverages the distinct strengths of different data types and AI paradigms. The classic ML models provide the deterministic, "hard" scores (e.g., "Complexity:

O(nlogn) \- Correct"), while the LLM provides the nuanced, human-like feedback that is crucial for a realistic training experience (e.g., "Readability: Good, but the variable temp\_list could be more descriptive."). This architecture results in a more robust, trustworthy, and pedagogically effective evaluation system.

**General "Big Code" Datasets:** The foundation of the evaluation engine can be built upon large-scale code corpora. Meta-repositories such as ml4code-dataset from CUHK-ARISE 52 and the

src-d/datasets collection 55 serve as invaluable indices, pointing to numerous specialized datasets. These collections include resources purpose-built for tasks like bug detection (OOPSLA19Li), vulnerability identification (Devign, Draper, Big-Vul), and code clone detection (BigCloneBench).52 These datasets provide millions of functions in languages like C, C++, and Java that have been labeled by static analysis tools or human experts. This data is essential for training models to recognize common code smells, logical errors, and security vulnerabilities, mirroring the rigor of a senior engineer's code review.

**Abstract Syntax Tree (AST) Datasets:** To enable a deeper, structural understanding of code, the AI must move beyond treating source code as mere text. Datasets composed of Abstract Syntax Trees (ASTs) are critical for this purpose. The py\_ast dataset available on Hugging Face, for example, provides 150,000 pre-parsed ASTs from a diverse set of Python programs.56 Academic research has demonstrated that models pretrained with an awareness of AST structure consistently outperform text-only models on tasks of code understanding and generation.57 By training on ASTs, the AI can learn to analyze logical flow, variable scope, and class structure, enabling it to provide much deeper feedback on a candidate's code design.

**Code Complexity Datasets:** A key task in any coding interview is the analysis of algorithmic complexity. The **CodeComplex** dataset is a uniquely valuable resource for training an AI to perform this task automatically.51 It contains 4,900 Java and 4,900 Python solutions to problems from the Codeforces platform. Crucially, each solution has been manually annotated by a panel of algorithmic experts with one of seven distinct worst-case time complexity classes, ranging from constant (

O(1)) to exponential. This purpose-built, expertly labeled dataset provides the ideal training ground for a model designed to predict the time complexity of a candidate's submitted solution, a core feature of the simulation platform.

**Software Metrics and "Clean Code" Heuristics:** Evaluating subjective qualities like readability and maintainability presents a different data challenge. While no single, large-scale dataset labeled for "clean vs. bad code" exists, the guiding principles are well-documented. Articles on clean coding practices 54 and forum discussions 61 outline key heuristics, such as the use of meaningful variable names, the creation of small, single-purpose functions, and the minimization of duplicative code. These qualitative rules can be synthesized into a detailed evaluation rubric. This rubric can then be used in two ways: first, as a guide for human annotators to label a smaller, high-quality dataset for fine-tuning a model; and second, as a core component of a zero-shot prompt for a powerful LLM tasked with code evaluation. To ground these subjective assessments, quantitative metrics from software mining datasets 62 can be used. These datasets, often derived from mining GitHub repositories, provide metrics like cyclomatic complexity, coupling between objects (CBO), and lack of cohesion in methods (LCOM), which can serve as strong quantitative proxies for code quality.

### **Section 2: System Design and Architectural Intelligence**

The system design interview is a cornerstone of Google's evaluation process for mid-level and senior engineers. It assesses a candidate's ability to navigate ambiguity, make reasoned trade-offs between competing constraints, and design scalable, resilient, and maintainable systems. Training an AI to simulate this open-ended, conversational interview requires a shift in data strategy. Instead of seeking vast quantities of labeled question-answer pairs, the focus must be on building a comprehensive knowledge base of architectural principles and patterns. This knowledge base will empower the AI to guide a realistic design discussion and evaluate the candidate's reasoning process.

#### **2.1 Structured System Design Case Studies**

True structured "datasets" for system design are exceedingly rare. The most valuable assets are curated collections of real-world interview prompts, which serve as the seeds for generating interview scenarios.

**Problem Databases:** The most significant find in this area is a Reddit thread that collates 45 system design questions reportedly asked at top tech companies, including Google.66 The value of this collection is threefold: each question is a realistic, high-level prompt (e.g., "Design a Distributed Metrics Logging and Aggregation System," "Design Google Drive"); each is tagged with the companies that have asked it, confirming its relevance; and the author provides a link to an external Airtable containing solution discussions. This list provides an excellent starting point for the AI's problem repository. Further searches on GitHub for topics like "system-design-questions" reveal numerous repositories that aggregate these and similar problems, confirming a canonical set of challenges that candidates are expected to master.67 This collection of prompts is ideal for fine-tuning an LLM to act as an interviewer, enabling it to generate novel variations of these classic problems and to understand the scope and key constraints of each.

#### **2.2 Real-World Architecture Precedents**

The "solution" to a system design problem is not a piece of code but a coherent architectural narrative that demonstrates an understanding of core principles. The data required to evaluate such a narrative is found in technical documentation, engineering blogs, and academic papers that describe how real-world systems are built.

The most effective AI for simulating a system design interview will not be a simple question-answering model. Such an approach would be brittle and incapable of handling the fluid, open-ended nature of a real design discussion. A more sophisticated and robust architecture involves an expert system that navigates a knowledge graph of architectural concepts. This reframes the data acquisition task from one of collecting interview transcripts to one of building and populating this knowledge graph.

1. System design interviews are not about recalling a single "correct" answer; they evaluate a candidate's thought process and their ability to connect disparate technical concepts to solve a problem under a set of constraints.  
2. The available data consists primarily of high-level prompts 66 and foundational architectural principles.68 There is no large, public dataset of "scored interview transcripts" from which a model can learn.  
3. Therefore, the optimal strategy is to structure the collected architectural principles into a formal knowledge graph. In this graph, nodes represent core concepts (e.g., "Load Balancer," "Consistent Hashing," "Sharding," "CAP Theorem"), and edges represent the relationships between them (e.g., "implements," "mitigates," "requires," "is a trade-off of").  
4. The AI interviewer would then use this graph to dynamically guide the conversation. When a candidate proposes a component, such as "scaling the database," the AI can traverse the graph from the "Database Scaling" node to ask relevant, context-aware follow-up questions about sharding, replication, or caching. The candidate's performance is then evaluated based on how effectively and logically they navigate this conceptual space, demonstrating their understanding of the interconnections and trade-offs. This approach transforms the problem from a simple NLP task into a more complex challenge of knowledge representation and automated reasoning.

**Knowledge Extraction from Technical Sources:** The primary data acquisition task is to build this knowledge base by extracting patterns, principles, and trade-offs from high-quality technical sources. This includes official documentation from cloud providers and database vendors, such as Oracle's detailed descriptions of distributed database features like sharding and replication.70 It also involves processing deep-dive articles and blog posts that explain scalable architecture patterns, such as the use of caching with Redis or Memcached, the role of load balancers, and the principles of data partitioning.68 Case studies from NoSQL database providers 71 and real-world examples like Twitter's sharding strategy or Amazon DynamoDB's replication model 68 provide concrete precedents. This knowledge must be extracted, structured, and encoded into the knowledge graph.

**Architecture Diagram Understanding:** A key component of modern system design is the ability to communicate ideas visually through architecture diagrams. The emergence of tools like DiagramGPT, which uses AI to generate diagrams from natural language descriptions, points to a future direction for the platform.72 While not a dataset in itself, this technology suggests a powerful interactive feature. An advanced version of the simulation platform could allow a candidate to describe their proposed architecture, with the AI generating a diagram in real-time for clarification. Conversely, the AI could present a diagram and ask the candidate to explain its components, trade-offs, and potential bottlenecks. This would require the AI to be trained not just on text but on a paired corpus of diagrams and their textual descriptions.

## **Part II: Datasets for Human-Centric Evaluation**

This part of the report addresses the acquisition of data for the more nuanced, human-centric components of the Google interview: the behavioral and cultural assessments. The data in this domain is predominantly unstructured, qualitative, and derived from subjective human experiences. Success hinges on a sophisticated data strategy focused on Natural Language Processing (NLP), knowledge engineering, and the synthesis of evaluation rubrics from disparate sources.

### **Section 3: Behavioral and Cultural Alignment Data**

The objective here is to train an AI that can conduct a behavioral interview, assess a candidate's soft skills and leadership potential, and evaluate their alignment with Google's culture. The core of this evaluation is the STAR method, which provides a structured framework for both answering and scoring responses.

#### **3.1 Aggregated Interview Experiences: Mining the Public Square**

The most authentic source of behavioral interview data comes from public forums where candidates share their experiences. Platforms like Glassdoor, Reddit, and Blind are invaluable repositories of real questions and anecdotal responses, but the data is unstructured, noisy, and requires careful processing.

**Glassdoor:** While Glassdoor does not offer an official public API for its interview question content 73, its value has prompted the creation of reliable third-party scraping tools. The scraper available on the Apify platform, for instance, can extract semi-structured data from company profiles, including interview questions, user-submitted experiences, difficulty ratings, and interview outcomes (e.g., received an offer, accepted, declined).76 Academic analyses of scraped Glassdoor data have demonstrated its utility for identifying hiring trends and patterns, validating its potential as a rich data source for this project.77

**Reddit and Blind:** These platforms offer more candid, narrative-style accounts of interview processes. Subreddits such as r/cscareerquestions are filled with detailed posts where users describe the questions they were asked, how they responded, and the feedback they received.78 The anonymous professional network Blind is another key source for such discussions. The

Blind-App-Reviews repository on GitHub provides a pre-scraped dataset of company reviews from Blind, which can be mined using keyword analysis to extract interview-related content.81

**Data Processing Pipeline:** The raw, unstructured nature of this data necessitates the development of a sophisticated NLP pipeline. The key stages of this pipeline must include:

1. **Question-Answer Extraction:** Identifying and isolating specific interview questions and the corresponding user-narrated answers or response summaries.  
2. **Competency Classification:** Classifying each extracted question according to a predefined taxonomy of behavioral competencies (e.g., "Teamwork," "Adaptability," "Leadership"), using the databases from university career centers as a guide.82  
3. **PII Anonymization:** A critical step to ensure legal and ethical compliance is the robust detection and redaction of all personally identifiable information (PII), such as names, specific project details, and locations.  
4. **Data Structuring:** Transforming the cleaned and classified data into a structured format, such as JSON, with distinct fields for the question, the company (if available), the role, and a summary of the user's experience or answer. The risks associated with scraping, such as potential IP blocks and the legal implications of using user-generated content, must be carefully managed throughout this process.84

#### **3.2 Frameworks for Soft Skill Assessment: From Questions to Rubrics**

The core of the behavioral AI is not just asking questions, but evaluating the answers. This requires moving beyond raw data to a structured framework of assessment.

The data acquisition strategy for this behavioral component must pivot. The search for massive, pre-scored interview datasets is futile, as they do not exist publicly. Instead, the strategy should be to create a smaller, exceptionally high-quality dataset of interview responses that have been expertly labeled against a synthesized, master rubric. The primary task for the AI is then to learn how to apply this rubric consistently and accurately at scale.

1. The goal is to evaluate behavioral interview answers.  
2. The universally accepted standard for a high-quality answer is the STAR method.85  
3. The specific criteria for what constitutes a good STAR response are well-documented across numerous academic and professional rubrics.87 These criteria consistently include completeness (addressing all four parts), specificity, demonstrating individual ownership ("I" vs. "we"), and quantifying the impact or result.  
4. No large public dataset of "10,000 STAR answers scored on a 1-5 scale for specificity" exists.  
5. Therefore, the most effective and defensible approach is to first synthesize a master evaluation rubric from all available high-quality sources.86  
6. This master rubric is then used to commission the creation of a high-quality, labeled dataset (e.g., 1,000-2,000 examples). In this dataset, human experts (e.g., trained HR professionals or senior engineers) score a diverse set of real and synthetic answers against the rubric's criteria.  
7. This expertly labeled dataset is then used to fine-tune an LLM. The model's objective is not merely to understand the content of the answer, but to internalize the principles of the rubric and learn to score new, unseen answers accordingly. This transforms the AI into a "Rubric Applier," which is a far more scalable, consistent, and strategically sound approach than relying on a massive, un-curated, and unlabeled dataset.

**Behavioral Question Databases:** The most valuable foundational assets are comprehensive databases of behavioral interview questions that have been categorized by the specific competency they are designed to assess. Documents from university career centers (e.g., University of Washington, University of Arkansas) and HR departments provide extensive lists of questions for competencies such as "Adaptability," "Communication," "Conflict Resolution," and "Decision Making".82 These resources provide a structured ontology that the AI can use to ensure it covers a comprehensive range of skills during a mock interview.

**The STAR Method as a Core Framework:** The research confirms that the STAR method (Situation, Task, Action, Result) is the industry-standard framework for structuring responses to behavioral questions.85 This framework is not just advice for candidates; it is the core evaluation model that interviewers use. Therefore, the AI's evaluation logic must be built around its ability to parse a candidate's response and identify these four components.

**Evaluation Rubrics:** The most critical data for training the AI evaluator are rubrics that define what constitutes a good STAR response. Several sources provide detailed criteria for this evaluation.86 For instance, a rubric from Northern Arizona University provides a three-tiered scoring system ("Does Not Meet," "Meets," "Exceeds") that evaluates the completeness of the STAR components, the relevance of the details, and the clarity of the writing.87 Another source emphasizes key evaluation points such as ensuring the candidate focuses on their individual contribution (using "I" instead of "we") and quantifies the results of their actions whenever possible.88 These rubrics provide the raw material for creating a detailed, multi-faceted scoring algorithm for the AI.

**Cultural Fit Frameworks:** To assess cultural fit beyond generic soft skills, the AI can be informed by established organizational psychology frameworks. The Organizational Culture Assessment Instrument (OCAI), based on the Competing Values Framework, provides a model for classifying corporate cultures into four archetypes: Clan (collaborative), Adhocracy (creative), Hierarchy (controlled), and Market (competitive).104 By analyzing Google's public values and employee testimonials, a profile of Google's culture can be created within this framework. The AI can then evaluate a candidate's preferences and past experiences to assess their alignment with this profile. More academic frameworks, such as the Leader-Culture Fit Framework, provide a theoretical basis for understanding the dynamic between an individual's capabilities and the prevailing organizational culture, offering a deeper model for the AI's assessment logic.105

### **Section 4: Emulating the "Googleyness" Factor**

To create a truly effective simulation, the AI must be calibrated to the specific nuances of Google's hiring philosophy. This involves moving beyond generic interview advice and integrating Google's own publicly documented standards for technical excellence, cultural fit, and problem-solving. This "ground truth" data is the most valuable and lowest-risk asset available, as it comes directly from the source.

#### **4.1 Analysis of Official Google Doctrine: The Ground Truth**

Google offers an unusually transparent view into its engineering culture and evaluation standards. This public documentation is a goldmine for establishing the core rubrics and principles that will govern the AI's evaluation logic.

**Hiring Process and Evaluation Rubrics:** The official Google Careers website and related resources provide a clear outline of the hiring process.106 More critically, multiple independent sources corroborate the structure of Google's internal interview evaluation rubric. This rubric consistently assesses candidates across four primary axes:

**Algorithms and Data Structures**, **Coding**, **Communication**, and **Problem-Solving**.108 The rubric detailed in 109 is particularly valuable, as it provides a specific 1-4 scoring guide for each category. For example, a score of 4 in "Coding" requires "working and clean code with no syntax errors" and an "outstanding understanding of paradigms," while a score of 2 indicates the candidate "struggled to produce the naive solution." This detailed rubric should be directly translated into the AI's scoring and feedback generation system. An alternative framework also cited is GCA (General Cognitive Ability), RRKE (Role-Related Knowledge and Experience), Leadership, and Googleyness, which provides a more holistic view of the candidate.110

**Deconstructing "Googleyness":** Far from being a vague buzzword, "Googleyness" is a well-defined set of cultural attributes. Authoritative sources consistently define it through traits such as **comfort with ambiguity**, a **bias for action**, strong **collaboration** skills, **intellectual humility**, and a **growth mindset**.111 These abstract traits can be made concrete by mapping them directly to the behavioral competencies and STAR method rubrics identified in Section 3.2. For example, a question about handling a project with shifting requirements directly assesses "comfort with ambiguity," and the quality of the STAR response can be scored against that specific trait.

**Engineering Practices and Code Review Standards:** Google's public Engineering Practices documentation, particularly the **Code Review Guidelines**, provides an unparalleled resource for defining what Google considers to be high-quality code.53 These guidelines go beyond simple style and delve into the core principles of software design at Google. They emphasize

**maintainability, readability, and simplicity over unnecessary complexity**. Key principles, such as "Technical facts and data overrule opinions and personal preferences" and the senior principle that reviewers should "favor approving a CL... even if the CL isn't perfect" as long as it improves overall code health, should be encoded into the AI's evaluation logic.53 This allows the AI to provide feedback that is not just technically correct but also culturally aligned with Google's engineering philosophy.

#### **4.2 AI Training and Synthetic Data Generation**

With a clear understanding of the evaluation criteria, the final step is to acquire or generate the necessary data to train the AI models. This involves leveraging general-purpose NLP datasets for foundational capabilities and employing advanced generative techniques to create specialized, proprietary training data where public sources are lacking.

**NLP for Communication Analysis:** The AI will need to process and understand the nuances of spoken language from mock interview transcripts to evaluate a candidate's communication skills. While interview-specific datasets are scarce, the AI's foundational language understanding can be built using large, general-purpose NLP datasets. Corpora for sentiment analysis 120, general text analysis 122, and conversational dialogue 124 can be used to pre-train the language models on the fundamentals of syntax, semantics, and discourse structure. This pre-training provides a strong base before the model is fine-tuned on the smaller, more specialized corpus of interview data.

**Performance Prediction:** While some datasets for predicting binary hiring outcomes exist, they are often synthetic or based on academic studies with limited generalizability.125 A more effective and strategically sound approach is to reframe the problem. Instead of predicting a simple "hire" or "no hire" decision, the AI's primary goal should be to predict a candidate's performance against Google's own detailed, multi-axis rubric.109 The output of the AI should not be a single bit of information but a rich, structured scorecard that mirrors the one a Google interviewer would complete, providing scores and justifications for each category (Algorithms, Coding, Communication, etc.). This approach is more actionable for the user and more directly aligned with the available ground-truth data.

**Synthetic Data Generation:** In domains where high-quality, labeled data is sparse or non-existent—such as system design solution paths or expertly scored behavioral answers—synthetic data generation becomes a critical strategic tool. While traditional tools like Gretel.ai and the Synthetic Data Vault (SDV) are excellent for generating structured, tabular data, the needs of this project are better met by LLM-based generative techniques.129 The

**"Evol-Instruct"** method, described in 131, is a particularly powerful strategy. This involves prompting a powerful LLM to iteratively generate new coding or system design problems with increasing complexity and diversity, starting from a small set of seed examples. This can create a vast, proprietary corpus of unique challenges. Similarly, AI-powered question generators can be used to create an extensive and varied pool of behavioral questions, ensuring that candidates are exposed to a wide range of scenarios.132

## **Part III: Strategic Implementation Framework**

This final part of the report synthesizes the detailed analysis of data sources into a concrete, actionable framework for implementation. It provides the master inventory of all identified data assets, outlines a phased strategy for their acquisition and integration, and concludes with a thorough assessment of the associated risks and proposed mitigation strategies. This framework is designed to serve as the guiding document for the engineering, product, and legal teams responsible for building the AI simulation platform.

### **Section 5: Comprehensive Dataset Inventory**

The following table represents the primary tactical deliverable of this analysis. It consolidates all identified data assets into a single, comprehensive inventory. This master table is designed to be a living document for the data acquisition and engineering teams, providing a centralized repository of knowledge that facilitates planning, prioritization, and the tracking of all potential data sources. By including metadata and strategic scoring for relevance, quality, accessibility, and risk, this inventory transforms a simple list of resources into a powerful tool for strategic decision-making. It provides clear, data-driven answers not only to the question of "What data is available?" but also to the more critical questions of "What data should be pursued first, what are the associated costs and risks, and what is the expected value to the platform?"

**Table 2: Comprehensive Dataset Inventory**

| Dataset Name/ID | Description | Source URL & Access Method | Data Format | Volume/Size | Google Relevance (1-5) | Data Quality (1-5) | Accessibility (1-5) | Legal/Ethical Risk | Integration Complexity | Priority Ranking |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| **LeetCode Google Questions** | Problems tagged with "Google" on LeetCode, including descriptions, solutions, and discussions. | Unofficial APIs (e.g., alfa-leetcode-api) / Direct Scraping. | JSON, HTML | \~2,000+ problems | 5 | 5 | 2 | **High** (ToS Violation) | High | **Phase 2** |
| **Kaggle LeetCode Problems** | Static snapshot of LeetCode problems with metadata like company tags, difficulty, and topics. | Kaggle Dataset Download 8 | CSV | \~1,800-2,200 problems | 4 | 4 | 5 | Low | Low | **Phase 1** |
| **Codeforces Problem Archive** | Complete archive of competitive programming problems, submissions, and test cases. | Official Public API / Scraping 18 | JSON | 10,000+ problems | 4 | 5 | 5 | Low | Low | **Phase 1** |
| **HackerRank for Work API** | Enterprise API for managing assessments; potential access to problem library. | Commercial/Partner API 33 | JSON | Large (Undisclosed) | 4 | 5 | 1 | Low (with contract) | Medium | **Phase 2 (Partner Track)** |
| **CodeComplex Dataset** | 9,800 Java/Python programs from Codeforces expertly annotated with time complexity. | GitHub Download 51 | JSON/Text | 9,800 programs | 5 | 5 | 5 | Low (Research License) | Low | **Phase 1** |
| **ml4code Datasets** | Meta-collection of academic datasets for bug detection, vulnerability analysis, and code classification. | Links via GitHub 52 | Various | Millions of functions | 4 | 4 | 5 | Low (Academic Use) | Medium | **Phase 1** |
| **py\_ast Hugging Face Dataset** | 150,000 Abstract Syntax Trees from Python programs on GitHub. | Hugging Face Hub 56 | JSON | 150,000 files | 3 | 4 | 5 | Low (MIT/BSD License) | Medium | **Phase 1** |
| **System Design Questions** | Curated list of 45 system design interview questions tagged by company. | Reddit / Airtable 66 | Text | 45 questions \+ solutions | 5 | 4 | 4 | Medium (Copyright) | Low | **Phase 1** |
| **Glassdoor Interview Data** | User-submitted interview questions, experiences, and outcomes. | Third-Party Scraper (e.g., Apify) 76 | JSON, CSV | 100,000s of reviews | 4 | 3 | 2 | **High** (ToS, PII) | High | **Phase 2** |
| **Behavioral Question Banks** | Comprehensive lists of behavioral questions categorized by competency. | University/HR Websites 82 | PDF, DOCX | \~1,000+ questions | 4 | 4 | 5 | Low | Low | **Phase 1** |
| **STAR Method Rubrics** | Documents detailing criteria for evaluating STAR method responses. | University/HR Websites 87 | PDF, DOCX | \~10-15 rubrics | 5 | 5 | 5 | Low | Low | **Phase 1** |
| **Google Official Docs** | Public documentation on hiring rubrics, code review standards, and "Googleyness." | Official Google Websites 53 | HTML | N/A | 5 | 5 | 5 | Low | Low | **Phase 1** |

### **Section 6: Phased Data Acquisition Strategy**

A phased approach to data acquisition is recommended to manage risk, align with product development milestones, and optimize resource allocation. This strategy prioritizes low-risk, high-impact data in the initial phase to enable rapid prototyping, followed by more complex and resource-intensive acquisition efforts to build a defensible, production-grade data asset.

#### **6.1 Phase 1 (Weeks 1-4): Foundational Model Prototyping**

The primary goal of this initial phase is to acquire a critical mass of high-quality, low-risk data to begin building and validating the core AI models immediately. This phase focuses exclusively on publicly available datasets and robust, well-documented APIs.

* **Actions:**  
  * **Algorithmic Problems:** Ingest all relevant static datasets from Kaggle that contain scraped LeetCode and other competitive programming problems.8 Simultaneously, build an integration with the official Codeforces public API to pull its complete problem archive and submission data.19  
  * **Code Evaluation:** Download and process the key academic datasets for code analysis. This includes the CodeComplex dataset for time complexity analysis 51, the  
    py\_ast dataset for structural code understanding 56, and the various bug detection and code quality corpora indexed by the  
    ml4code-dataset repository.52  
  * **Behavioral & Google-Specific Frameworks:** Scrape and parse all available public documentation from Google's engineering and careers sites to build the "ground truth" knowledge base for evaluation rubrics.53 Concurrently, collect and structure the behavioral question databases and STAR method evaluation rubrics from university and HR sources.82  
* **Goal:** By the end of this phase, the engineering team will possess a rich, structured dataset sufficient to build and validate initial prototype models for code complexity analysis, problem categorization, and rubric-based evaluation of both code and behavioral responses.

#### **6.2 Phase 2 (Weeks 5-12): Building the Production Pipeline**

This phase focuses on developing the infrastructure required for a production system that relies on dynamic, fresh data. The emphasis shifts to building resilient scraping frameworks and initiating formal partnership discussions.

* **Actions:**  
  * **Develop Scraping Infrastructure:** Architect and implement an ethical, robust scraping framework. The primary target is LeetCode, specifically to extract the Google-tagged questions and associated solutions. Secondary targets include the HackerRank Interview Preparation Kit and the user experience forums on Reddit and Blind. This framework must include sophisticated rate limiting, user-agent rotation, handling of dynamic web content, and a robust PII detection and anonymization module for user-generated content.  
  * **Initiate Partnership Outreach:** Begin formal discussions with platforms that have commercial APIs, with HackerRank being the top priority. The goal is to explore data licensing or partnership agreements that would provide sanctioned access to their problem libraries.  
* **Goal:** To create a continuously operating data pipeline that ingests fresh data from key sources, ensuring the platform's content remains current and relevant. A secondary goal is to assess the viability and cost of data partnerships as an alternative to scraping.

#### **6.3 Phase 3 (Ongoing): Scaling with Synthetic Data and User Contributions**

The final phase is an ongoing effort focused on long-term strategic advantage. It aims to reduce reliance on third-party sources, create unique and proprietary data assets, and cover edge cases not found in publicly available data.

* **Actions:**  
  * **Implement Synthetic Generation:** Develop LLM-based data generation pipelines. Use techniques like Evol-Instruct 131 to create a large, proprietary corpus of novel coding problems and system design scenarios.  
  * **Create Expert-Labeled Datasets:** Commission a small team of experts (e.g., former Google engineers, professional interview coaches) to create a high-quality, labeled dataset of scored STAR method responses. This "golden" dataset will be used to fine-tune the behavioral evaluation model for unparalleled accuracy.  
  * **Explore User-Permissioned Data:** Investigate and potentially implement features that allow users to securely connect their own accounts (e.g., LeetCode, GitHub) via OAuth. This would enable the platform to access their submission history and provide personalized feedback, while also serving as a highly valuable—and fully permissioned—source of training data.  
* **Goal:** To build a defensible data moat based on proprietary, synthetically generated content and user-contributed data, ensuring the platform's long-term competitive advantage and reducing legal and technical risks associated with external data sources.

### **Section 7: Risk Assessment and Mitigation**

A proactive approach to risk management is essential, given the complex data landscape. The primary risks fall into three categories: legal and ethical compliance, data quality and relevance, and technical implementation hurdles.

#### **7.1 Legal and Ethical Compliance**

* **Risk:** The most significant risk is the potential violation of website Terms of Service (ToS), particularly from LeetCode, which explicitly restricts the reproduction of its content.6 This could lead to legal action, including cease-and-desist orders or lawsuits. A secondary risk is copyright infringement on the creative expression within problem statements. Finally, scraping user-generated content from forums like Reddit and Glassdoor carries a high risk of mishandling personally identifiable information (PII), which could violate data privacy regulations like GDPR.13  
* **Mitigation:**  
  1. **Legal Counsel:** Engage legal counsel with expertise in intellectual property and data privacy *before* initiating any web scraping activities.  
  2. **Internal Use Policy:** Establish and enforce a strict internal policy that all scraped data is to be used exclusively for internal model training and is never to be displayed directly to end-users.  
  3. **Robust Anonymization:** Develop, test, and audit a state-of-the-art PII detection and anonymization pipeline for all user-generated content.  
  4. **Prioritize Permissive Licenses:** In all development phases, prioritize the use of data from sources with clear, permissive licenses, such as the academic datasets identified in Phase 1\.

#### **7.2 Data Quality and Relevance**

* **Risk:** The data's relevance can decay over time. Static datasets from Kaggle will quickly become outdated as Google's interview practices evolve.9 User-generated content from forums can be inaccurate, incomplete, or subject to selection bias. The definition of a "Google question" is fluid and can change.  
* **Mitigation:**  
  1. **Dynamic Pipelines:** Mitigate data staleness by implementing the dynamic scraping and API integration pipelines outlined in Phase 2\.  
  2. **Cross-Validation:** Use multiple data sources to cross-validate information. For example, if a problem is tagged as a "Google question" on both LeetCode and in multiple Glassdoor reviews, confidence in its relevance is much higher.  
  3. **Continuous Monitoring:** Assign a resource to continuously monitor official Google hiring resources, engineering blogs, and conference talks for any announced changes in interview focus or patterns. This intelligence should be used to update the "ground truth" knowledge base and re-weight the relevance of existing data.

#### **7.3 Technical and Integration Hurdles**

* **Risk:** The technical infrastructure for data acquisition is fraught with potential failure points. Unofficial APIs are brittle by nature and can break without warning, disrupting the data pipeline.2 Aggressive or poorly designed scrapers can lead to temporary or permanent IP address bans. The process of ingesting, cleaning, and harmonizing data from dozens of sources with different schemas is a significant and ongoing engineering effort.  
* **Mitigation:**  
  1. **Modular Ingestion Layer:** Design a modular data ingestion architecture with distinct "adapters" for each data source. This isolates failures and allows for the easy addition, removal, or modification of sources without affecting the entire pipeline.  
  2. **Sophisticated Scraping Practices:** Implement best-in-class scraping techniques, including distributed proxies, user-agent rotation, and adaptive rate limiting that responds to server load.  
  3. **Monitoring and Alerting:** Build comprehensive monitoring and alerting systems for the entire data acquisition infrastructure to rapidly detect and respond to API changes, scraper failures, or IP blocks.  
  4. **Canonical Data Model:** Define a canonical, internal data model for each data type (e.g., a "Problem" object, a "BehavioralResponse" object). All data ingested from external sources should be transformed into this standard model, simplifying downstream processing and model training.

#### **Works cited**

1. Top Google Questions \- LeetCode, accessed July 29, 2025, [https://leetcode.com/problem-list/7p55wqm/](https://leetcode.com/problem-list/7p55wqm/)  
2. Unofficial LeetCode API \+ Daily Updated Google Sheet \- DEV ..., accessed July 29, 2025, [https://dev.to/noworneverev/unofficial-leetcode-api-daily-updated-google-sheet-179o](https://dev.to/noworneverev/unofficial-leetcode-api-daily-updated-google-sheet-179o)  
3. alfaarghya/alfa-leetcode-api: It's a custom leetcode api ... \- GitHub, accessed July 29, 2025, [https://github.com/alfaarghya/alfa-leetcode-api](https://github.com/alfaarghya/alfa-leetcode-api)  
4. jrluu/Leetcode-Scraper: LeetCode Scraper is a python program that takes advantage of selenium to logins into a user's LeetCode account, and scrape code for their solved problems. It stores their code inside of a file of their choosing. \- GitHub, accessed July 29, 2025, [https://github.com/jrluu/Leetcode-Scraper](https://github.com/jrluu/Leetcode-Scraper)  
5. Bishalsarang/Leetcode-Questions-Scraper \- GitHub, accessed July 29, 2025, [https://github.com/Bishalsarang/Leetcode-Questions-Scraper](https://github.com/Bishalsarang/Leetcode-Questions-Scraper)  
6. Leetcode scraping legal/illegal \- Grapevine, accessed July 29, 2025, [https://www.grapevine.in/post/leetcode-scraping-legal-illegal-ece69027-7d3f-426c-b78b-77851ff00c38](https://www.grapevine.in/post/leetcode-scraping-legal-illegal-ece69027-7d3f-426c-b78b-77851ff00c38)  
7. The LeetCode Solution Dataset \- Kaggle, accessed July 29, 2025, [https://www.kaggle.com/datasets/eemanmajumder/the-leetcode-solution-dataset](https://www.kaggle.com/datasets/eemanmajumder/the-leetcode-solution-dataset)  
8. Leetcode all problems dataset \- Kaggle, accessed July 29, 2025, [https://www.kaggle.com/datasets/nehagupta09/leetcode-all-problems-dataset](https://www.kaggle.com/datasets/nehagupta09/leetcode-all-problems-dataset)  
9. Leetcode Problem Dataset \- Kaggle, accessed July 29, 2025, [https://www.kaggle.com/datasets/gzipchrist/leetcode-problem-dataset](https://www.kaggle.com/datasets/gzipchrist/leetcode-problem-dataset)  
10. Leetcode Questions Dataset \- Kaggle, accessed July 29, 2025, [https://www.kaggle.com/datasets/mohitkumar282/leetcode-questions-dataset](https://www.kaggle.com/datasets/mohitkumar282/leetcode-questions-dataset)  
11. Leetcode Solutions and Content KPIs \- Kaggle, accessed July 29, 2025, [https://www.kaggle.com/datasets/jacobhds/leetcode-solutions-and-content-kpis](https://www.kaggle.com/datasets/jacobhds/leetcode-solutions-and-content-kpis)  
12. Scrape-LeetCode-Solutions/leetcode\_scraper.py at master \- GitHub, accessed July 29, 2025, [https://github.com/prateekiiest/Scrape-LeetCode-Solutions/blob/master/leetcode\_scraper.py](https://github.com/prateekiiest/Scrape-LeetCode-Solutions/blob/master/leetcode_scraper.py)  
13. Ethical Web Scraping: Principles and Practices \- DataCamp, accessed July 29, 2025, [https://www.datacamp.com/blog/ethical-web-scraping](https://www.datacamp.com/blog/ethical-web-scraping)  
14. Ethics & Legality of Webscraping \- UCSB Carpentry, accessed July 29, 2025, [https://carpentry.library.ucsb.edu/2022-05-12-ucsb-webscraping/06-Ethics-Legality-Webscraping/index.html](https://carpentry.library.ucsb.edu/2022-05-12-ucsb-webscraping/06-Ethics-Legality-Webscraping/index.html)  
15. Web Scraping Ethics: Adhering to Legal and Ethical Guidelines \- MoldStud, accessed July 29, 2025, [https://moldstud.com/articles/p-web-scraping-ethics-adhering-to-legal-and-ethical-guidelines](https://moldstud.com/articles/p-web-scraping-ethics-adhering-to-legal-and-ethical-guidelines)  
16. Legal and Ethical Considerations \- Python Web Scraping \- Monash Data Fluency, accessed July 29, 2025, [https://monashdatafluency.github.io/python-web-scraping/section-5-legal-and-ethical-considerations/](https://monashdatafluency.github.io/python-web-scraping/section-5-legal-and-ethical-considerations/)  
17. Guideline to Legal and Ethical Web Scraping | by X-Byte Enterprise Crawling | Medium, accessed July 29, 2025, [https://medium.com/@xbytecrawling/guideline-to-legal-and-ethical-web-scraping-1180e501e28c](https://medium.com/@xbytecrawling/guideline-to-legal-and-ethical-web-scraping-1180e501e28c)  
18. COFO: COdeFOrces dataset for Program Classification, Recognition and Tagging \- arXiv, accessed July 29, 2025, [https://arxiv.org/html/2503.18251v1](https://arxiv.org/html/2503.18251v1)  
19. Codeforces.API \- Hackage, accessed July 29, 2025, [https://hackage.haskell.org/package/codeforces-cli/docs/Codeforces-API.html](https://hackage.haskell.org/package/codeforces-cli/docs/Codeforces-API.html)  
20. Codeforces API — Free Public API | Public APIs Directory, accessed July 29, 2025, [https://publicapis.io/codeforces-api](https://publicapis.io/codeforces-api)  
21. Codeforces API \- PublicAPI, accessed July 29, 2025, [https://publicapi.dev/codeforces-api](https://publicapi.dev/codeforces-api)  
22. Dataset Card for CodeForces \- ModelScope, accessed July 29, 2025, [https://modelscope.cn/datasets/AI-ModelScope/Codeforces](https://modelscope.cn/datasets/AI-ModelScope/Codeforces)  
23. API Terms of Use | Klaviyo Legal, accessed July 29, 2025, [https://www.klaviyo.com/legal/api-terms](https://www.klaviyo.com/legal/api-terms)  
24. Mukundan314/python-codeforces: Codeforces API wrapper for python \- GitHub, accessed July 29, 2025, [https://github.com/Mukundan314/python-codeforces](https://github.com/Mukundan314/python-codeforces)  
25. rishiiiidha/codeforces-api \- GitHub, accessed July 29, 2025, [https://github.com/rishiiiidha/codeforces-api](https://github.com/rishiiiidha/codeforces-api)  
26. COFO: COdeFOrces dataset for Program Classification, Recognition and Tagging \- arXiv, accessed July 29, 2025, [https://arxiv.org/html/2503.18251](https://arxiv.org/html/2503.18251)  
27. Qwen3-Coder is Finally Here and It's Breaking All the Coding Benchmarks \- Apidog, accessed July 29, 2025, [https://apidog.com/blog/qwen3-coder/](https://apidog.com/blog/qwen3-coder/)  
28. Introducing Claude 3.5 Sonnet \- Anthropic, accessed July 29, 2025, [https://www.anthropic.com/news/claude-3-5-sonnet](https://www.anthropic.com/news/claude-3-5-sonnet)  
29. API Terms of Service \- Relay, accessed July 29, 2025, [https://relaypro.com/legal/relay-api-terms-of-service/](https://relaypro.com/legal/relay-api-terms-of-service/)  
30. Criteo API Terms and Conditions, accessed July 29, 2025, [https://developers.criteo.com/marketing-solutions/docs/criteo-api-terms-and-conditions](https://developers.criteo.com/marketing-solutions/docs/criteo-api-terms-and-conditions)  
31. Claude 3.7 Sonnet and Claude Code \- Anthropic, accessed July 29, 2025, [https://www.anthropic.com/news/claude-3-7-sonnet](https://www.anthropic.com/news/claude-3-7-sonnet)  
32. CodeforcesAPI/codeforces/api/codeforces\_api.py at master \- GitHub, accessed July 29, 2025, [https://github.com/soon/CodeforcesAPI/blob/master/codeforces/api/codeforces\_api.py](https://github.com/soon/CodeforcesAPI/blob/master/codeforces/api/codeforces_api.py)  
33. HackerRank for Work API, accessed July 29, 2025, [https://www.hackerrank.com/work/apidocs](https://www.hackerrank.com/work/apidocs)  
34. HackerRank for Work API | Documentation | Postman API Network, accessed July 29, 2025, [https://www.postman.com/blue-space-686402/hackerrank-for-work-api/documentation/ti5uc2d/hackerrank-for-work-api](https://www.postman.com/blue-space-686402/hackerrank-for-work-api/documentation/ti5uc2d/hackerrank-for-work-api)  
35. HackerRank for Work API | Get Started \- Postman, accessed July 29, 2025, [https://www.postman.com/blue-space-686402/hackerrank-for-work-api/collection/ti5uc2d/hackerrank-for-work-api](https://www.postman.com/blue-space-686402/hackerrank-for-work-api/collection/ti5uc2d/hackerrank-for-work-api)  
36. HackerRank | REST API documentation and SDKs \- RESTUnited, accessed July 29, 2025, [http://restunited.com/docs/3bqbx05520d0](http://restunited.com/docs/3bqbx05520d0)  
37. Resources \- HackerRank, accessed July 29, 2025, [https://www.hackerrank.com/resources/](https://www.hackerrank.com/resources/)  
38. HackerRank Terms of Service, accessed July 29, 2025, [https://www.hackerrank.com/terms-of-service/](https://www.hackerrank.com/terms-of-service/)  
39. Rest API (Intermediate) Skills Certification Test \- HackerRank, accessed July 29, 2025, [https://www.hackerrank.com/skills-verification/rest\_api\_intermediate](https://www.hackerrank.com/skills-verification/rest_api_intermediate)  
40. Pricing-terms \- HackerRank, accessed July 29, 2025, [https://www.hackerrank.com/about-us/pricing-terms](https://www.hackerrank.com/about-us/pricing-terms)  
41. The HackerRank Interview Preparation Kit, accessed July 29, 2025, [https://www.hackerrank.com/interview/interview-preparation-kit](https://www.hackerrank.com/interview/interview-preparation-kit)  
42. qatadaazzeh/atcoder-api \- NPM, accessed July 29, 2025, [https://www.npmjs.com/package/@qatadaazzeh/atcoder-api](https://www.npmjs.com/package/@qatadaazzeh/atcoder-api)  
43. atcoder-api-python \- Read the Docs Community, accessed July 29, 2025, [https://readthedocs.org/projects/atcoder-api-python/](https://readthedocs.org/projects/atcoder-api-python/)  
44. iamsmruti/codechef-api-unofficial \- GitHub, accessed July 29, 2025, [https://github.com/iamsmruti/codechef-api-unofficial](https://github.com/iamsmruti/codechef-api-unofficial)  
45. harshit-budhraja/codechef-api \- GitHub, accessed July 29, 2025, [https://github.com/harshit-budhraja/codechef-api](https://github.com/harshit-budhraja/codechef-api)  
46. Practice Problems \- CodeChef, accessed July 29, 2025, [https://www.codechef.com/practice-old](https://www.codechef.com/practice-old)  
47. CodeChef \- Learn and Practice Coding with Problems, accessed July 29, 2025, [https://www.codechef.com/](https://www.codechef.com/)  
48. Topcoder: Home, accessed July 29, 2025, [https://www.topcoder.com/](https://www.topcoder.com/)  
49. Top Coder | PDF | Algorithms | Data Compression \- Scribd, accessed July 29, 2025, [https://www.scribd.com/doc/36884347/Top-Coder](https://www.scribd.com/doc/36884347/Top-Coder)  
50. competitive programming arena \- Topcoder, accessed July 29, 2025, [https://www.topcoder.com/community/arena](https://www.topcoder.com/community/arena)  
51. sybaik1/CodeComplex: Testing SOTA models on ... \- GitHub, accessed July 29, 2025, [https://github.com/sybaik1/CodeComplex-Models](https://github.com/sybaik1/CodeComplex-Models)  
52. CUHK-ARISE/ml4code-dataset: A collection of datasets for ... \- GitHub, accessed July 29, 2025, [https://github.com/CUHK-ARISE/ml4code-dataset](https://github.com/CUHK-ARISE/ml4code-dataset)  
53. The Standard of Code Review | eng-practices \- Google, accessed July 29, 2025, [https://google.github.io/eng-practices/review/reviewer/standard.html](https://google.github.io/eng-practices/review/reviewer/standard.html)  
54. A Simple Guide to Clean Coding: How to Write and Review Better Software \- Medium, accessed July 29, 2025, [https://medium.com/@dinesharney/a-simple-guide-to-clean-coding-how-to-write-and-review-better-software-7b2cdc3adc91](https://medium.com/@dinesharney/a-simple-guide-to-clean-coding-how-to-write-and-review-better-software-7b2cdc3adc91)  
55. GitHub \- src-d/datasets, accessed July 29, 2025, [https://github.com/src-d/datasets](https://github.com/src-d/datasets)  
56. 1stvamp/py\_ast · Datasets at Hugging Face, accessed July 29, 2025, [https://huggingface.co/datasets/1stvamp/py\_ast](https://huggingface.co/datasets/1stvamp/py_ast)  
57. AST-T5: Structure-Aware Pretraining for Code Generation and Understanding \- arXiv, accessed July 29, 2025, [https://arxiv.org/html/2401.03003v1](https://arxiv.org/html/2401.03003v1)  
58. CodeComplex: A Time-Complexity Dataset for Bilingual Source Codes \- arXiv, accessed July 29, 2025, [https://arxiv.org/html/2401.08719v1](https://arxiv.org/html/2401.08719v1)  
59. CodeComplex: A Time-complexity Dataset for Multi-language Source Codes \- OpenReview, accessed July 29, 2025, [https://openreview.net/forum?id=8tGu1pNUnN](https://openreview.net/forum?id=8tGu1pNUnN)  
60. How to Write Clean Code in Python | Towards Data Science, accessed July 29, 2025, [https://towardsdatascience.com/how-to-write-clean-code-in-python-d1ffb9d9f042/](https://towardsdatascience.com/how-to-write-clean-code-in-python-d1ffb9d9f042/)  
61. Need Help With An Example of Clean, Elegant code VS Bad, Messy Code, accessed July 29, 2025, [https://forum.freecodecamp.org/t/need-help-with-an-example-of-clean-elegant-code-vs-bad-messy-code/583563](https://forum.freecodecamp.org/t/need-help-with-an-example-of-clean-elegant-code-vs-bad-messy-code/583563)  
62. A Directory of Datasets for Mining Software Repositories \- MDPI, accessed July 29, 2025, [https://www.mdpi.com/2306-5729/10/3/28](https://www.mdpi.com/2306-5729/10/3/28)  
63. Tools For Code Analysis and Mining | Code Analysis Toolkit SDK \- GitHub Pages, accessed July 29, 2025, [https://cat-sdk.github.io/](https://cat-sdk.github.io/)  
64. visminer/repositoryminer \- GitHub, accessed July 29, 2025, [https://github.com/visminer/repositoryminer](https://github.com/visminer/repositoryminer)  
65. Code Metrics Dataset SoftwareProjectStructure \- Kaggle, accessed July 29, 2025, [https://www.kaggle.com/datasets/amalsalilan/code-metrics-dataset-softwareprojectstructure](https://www.kaggle.com/datasets/amalsalilan/code-metrics-dataset-softwareprojectstructure)  
66. 45 system design questions I curated for interviews : r/leetcode, accessed July 29, 2025, [https://www.reddit.com/r/leetcode/comments/1j9a8u6/45\_system\_design\_questions\_i\_curated\_for/](https://www.reddit.com/r/leetcode/comments/1j9a8u6/45_system_design_questions_i_curated_for/)  
67. system-design-questions · GitHub Topics, accessed July 29, 2025, [https://github.com/topics/system-design-questions](https://github.com/topics/system-design-questions)  
68. Building a Scalable Database \- GeeksforGeeks, accessed July 29, 2025, [https://www.geeksforgeeks.org/dbms/building-a-scalable-database/](https://www.geeksforgeeks.org/dbms/building-a-scalable-database/)  
69. Database Scaling Patterns. As an application grows from a single… | by Aayushvlad | Medium, accessed July 29, 2025, [https://medium.com/@aayushvlad/database-scaling-patterns-62ff9619efec](https://medium.com/@aayushvlad/database-scaling-patterns-62ff9619efec)  
70. Globally Distributed Database | Oracle, accessed July 29, 2025, [https://www.oracle.com/database/distributed-database/](https://www.oracle.com/database/distributed-database/)  
71. Case Studies On NoSQL \- Meegle, accessed July 29, 2025, [https://www.meegle.com/en\_us/topics/nosql/case-studies-on-nosql](https://www.meegle.com/en_us/topics/nosql/case-studies-on-nosql)  
72. DiagramGPT – AI diagram generator created by Eraser \- Eraser IO, accessed July 29, 2025, [https://www.eraser.io/diagramgpt](https://www.eraser.io/diagramgpt)  
73. Top 50 API Testing Interview Questions and Answers (2025) \- Simplilearn.com, accessed July 29, 2025, [https://www.simplilearn.com/top-api-testing-interview-questions-article](https://www.simplilearn.com/top-api-testing-interview-questions-article)  
74. Top API Testing Interview Questions (2025) \- InterviewBit, accessed July 29, 2025, [https://www.interviewbit.com/api-testing-interview-questions/](https://www.interviewbit.com/api-testing-interview-questions/)  
75. 2025 API Developer Interview Questions & Answers (Top Ranked) \- Teal, accessed July 29, 2025, [https://www.tealhq.com/interview-questions/api-developer](https://www.tealhq.com/interview-questions/api-developer)  
76. Glassdoor Reviews, Interviews, Locations, Salary, Job, Overview ..., accessed July 29, 2025, [https://apify.com/memo23/apify-glassdoor-reviews-scraper](https://apify.com/memo23/apify-glassdoor-reviews-scraper)  
77. What Glassdoor interview reviews reveal about tech hiring cultures \- Chip Huyen, accessed July 29, 2025, [https://huyenchip.com/2019/08/21/glassdoor-interview-reviews-tech-hiring-cultures.html](https://huyenchip.com/2019/08/21/glassdoor-interview-reviews-tech-hiring-cultures.html)  
78. Meta Is Going to Let Job Candidates Use AI During Coding Tests : r/cscareerquestions, accessed July 29, 2025, [https://www.reddit.com/r/cscareerquestions/comments/1mcb3hg/meta\_is\_going\_to\_let\_job\_candidates\_use\_ai\_during/](https://www.reddit.com/r/cscareerquestions/comments/1mcb3hg/meta_is_going_to_let_job_candidates_use_ai_during/)  
79. Computer Science Career Questions \- Reddit, accessed July 29, 2025, [https://www.reddit.com/r/cscareerquestions/](https://www.reddit.com/r/cscareerquestions/)  
80. Worst/Best/Funny interview experiences? : r/cscareerquestions \- Reddit, accessed July 29, 2025, [https://www.reddit.com/r/cscareerquestions/comments/3psvdc/worstbestfunny\_interview\_experiences/](https://www.reddit.com/r/cscareerquestions/comments/3psvdc/worstbestfunny_interview_experiences/)  
81. Scraped reviews of over 25 companies from the Blind App ⚡️ \- GitHub, accessed July 29, 2025, [https://github.com/HarshCasper/Blind-App-Reviews](https://github.com/HarshCasper/Blind-App-Reviews)  
82. Behavioral-Interview-Question-Inventory-by-Competency-20230213.docx, accessed July 29, 2025, [https://hr.uw.edu/talent/wp-content/uploads/sites/17/2023/02/Behavioral-Interview-Question-Inventory-by-Competency-20230213.docx](https://hr.uw.edu/talent/wp-content/uploads/sites/17/2023/02/Behavioral-Interview-Question-Inventory-by-Competency-20230213.docx)  
83. Extensive List of Competency-Based Behavioral Interview Questions, accessed July 29, 2025, [https://walton.uark.edu/career/files\_career\_center/Extensive\_List\_of\_Competency-Based\_Interview\_Questions.pdf](https://walton.uark.edu/career/files_career_center/Extensive_List_of_Competency-Based_Interview_Questions.pdf)  
84. SQL Injection \- GeeksforGeeks, accessed July 29, 2025, [https://www.geeksforgeeks.org/sql/sql-injection/](https://www.geeksforgeeks.org/sql/sql-injection/)  
85. STAR Strategy Examples | Career Center | Case Western Reserve University, accessed July 29, 2025, [https://case.edu/studentlife/careercenter/career-development/career-resources/tips-job-seekers/interviewing/behavior-based-interviewing/star-strategy-examples](https://case.edu/studentlife/careercenter/career-development/career-resources/tips-job-seekers/interviewing/behavior-based-interviewing/star-strategy-examples)  
86. The STAR method | National Careers Service, accessed July 29, 2025, [https://nationalcareers.service.gov.uk/careers-advice/interview-advice/the-star-method](https://nationalcareers.service.gov.uk/careers-advice/interview-advice/the-star-method)  
87. Rubric: STAR Behavioral Interview Questions \- in.nau.edu, accessed July 29, 2025, [https://in.nau.edu/wp-content/uploads/sites/204/2018/06/STAR-Behavioral-Interview-Assignment-Rubric.pdf](https://in.nau.edu/wp-content/uploads/sites/204/2018/06/STAR-Behavioral-Interview-Assignment-Rubric.pdf)  
88. The STAR Method of Behavioral Interviewing \- VA Wizard, accessed July 29, 2025, [https://www.vawizard.org/wiz-pdf/STAR\_Method\_Interviews.pdf](https://www.vawizard.org/wiz-pdf/STAR_Method_Interviews.pdf)  
89. STAR Method for Interviewing and Feedback \- DDI, accessed July 29, 2025, [https://www.ddiworld.com/solutions/behavioral-interviewing/star-method](https://www.ddiworld.com/solutions/behavioral-interviewing/star-method)  
90. Rating Interview Responses using the STAR method \- The Predictive Index, accessed July 29, 2025, [https://www.predictiveindex.com/learn/support/rating-interview-responses-using-the-star-method/](https://www.predictiveindex.com/learn/support/rating-interview-responses-using-the-star-method/)  
91. How to assess the responses to behavioral interview questions? : r/IOPsychology \- Reddit, accessed July 29, 2025, [https://www.reddit.com/r/IOPsychology/comments/10m0ong/how\_to\_assess\_the\_responses\_to\_behavioral/](https://www.reddit.com/r/IOPsychology/comments/10m0ong/how_to_assess_the_responses_to_behavioral/)  
92. 30 star method interview questions to prepare for \- BetterUp, accessed July 29, 2025, [https://www.betterup.com/blog/star-interview-method](https://www.betterup.com/blog/star-interview-method)  
93. The Best 5 Steps to Analyze STAR Method of Behavioral Interviewing Responses \- Risely, accessed July 29, 2025, [https://www.risely.me/star-method-of-behavioral-interviewing/](https://www.risely.me/star-method-of-behavioral-interviewing/)  
94. Behavioral Interview Questions, accessed July 29, 2025, [https://www.hr.utah.edu/forms/lib/Behavioral\_Interview\_Questions.pdf](https://www.hr.utah.edu/forms/lib/Behavioral_Interview_Questions.pdf)  
95. 21 Behavioral Interview Questions and Answers | Coursera, accessed July 29, 2025, [https://www.coursera.org/articles/behavioral-interview-questions](https://www.coursera.org/articles/behavioral-interview-questions)  
96. List Of Behavioral Interview Questions For Database Managers \- Poised, accessed July 29, 2025, [https://www.poised.com/blog/list-of-behavioral-interview-questions-for-database-managers](https://www.poised.com/blog/list-of-behavioral-interview-questions-for-database-managers)  
97. Using the STAR method for your next behavioral interview (worksheet included), accessed July 29, 2025, [https://capd.mit.edu/resources/the-star-method-for-behavioral-interviews/](https://capd.mit.edu/resources/the-star-method-for-behavioral-interviews/)  
98. Behavioral Questions by Job Competency \- UNM Human Resources, accessed July 29, 2025, [https://hr.unm.edu/docs/employment/behavioral-questions-by-job-competency-for-job-interviews-(pdf).pdf](https://hr.unm.edu/docs/employment/behavioral-questions-by-job-competency-for-job-interviews-\(pdf\).pdf)  
99. 20+ Competency-Based Interview Questions (+ How To Answer), accessed July 29, 2025, [https://resources.biginterview.com/behavioral-interviews/competency-based-interview-questions/](https://resources.biginterview.com/behavioral-interviews/competency-based-interview-questions/)  
100. Interview Questions Database | People & Culture \- UC Berkeley, accessed July 29, 2025, [https://hr.berkeley.edu/grow/grow-your-resources/uc-systemwide-core-competency-abcs/interview-questions-database](https://hr.berkeley.edu/grow/grow-your-resources/uc-systemwide-core-competency-abcs/interview-questions-database)  
101. STAR Interview Questions: What They Are \+ How to Answer Them \- Coursera, accessed July 29, 2025, [https://www.coursera.org/articles/star-interview-questions](https://www.coursera.org/articles/star-interview-questions)  
102. 27+ STAR Interview Questions & Answers in 2025 \- Novoresume, accessed July 29, 2025, [https://novoresume.com/career-blog/star-interview-questions](https://novoresume.com/career-blog/star-interview-questions)  
103. STAR Method: How to Use This Technique to Ace Your Next Job Interview \- The Muse, accessed July 29, 2025, [https://www.themuse.com/advice/star-interview-method](https://www.themuse.com/advice/star-interview-method)  
104. Cultural Fit Assessment \- Harver, accessed July 29, 2025, [https://harver.com/assessments/cultural-fit-assessment/](https://harver.com/assessments/cultural-fit-assessment/)  
105. (PDF) ALIGNING LEADERSHIP AND ORGANIZATIONAL CULTURE ..., accessed July 29, 2025, [https://www.researchgate.net/publication/263938182\_ALIGNING\_LEADERSHIP\_AND\_ORGANIZATIONAL\_CULTURE\_THE\_LEADER-CULTURE\_FIT\_FRAMEWORK\_FOR\_COACHING\_ORGANIZATIONAL\_LEADERS](https://www.researchgate.net/publication/263938182_ALIGNING_LEADERSHIP_AND_ORGANIZATIONAL_CULTURE_THE_LEADER-CULTURE_FIT_FRAMEWORK_FOR_COACHING_ORGANIZATIONAL_LEADERS)  
106. Apply for a job \- Google Careers Help, accessed July 29, 2025, [https://support.google.com/googlecareers/answer/6095391?hl=en](https://support.google.com/googlecareers/answer/6095391?hl=en)  
107. Our hiring process \- Google Careers, accessed July 29, 2025, [https://www.google.com/about/careers/applications/how-we-hire/](https://www.google.com/about/careers/applications/how-we-hire/)  
108. How candidates are evaluated in coding interviews at top tech companies, accessed July 29, 2025, [https://www.techinterviewhandbook.org/coding-interview-rubrics/](https://www.techinterviewhandbook.org/coding-interview-rubrics/)  
109. Google Coding Interview Rubric—An Inside Look \- Exponent, accessed July 29, 2025, [https://www.tryexponent.com/blog/google-coding-interview-rubric](https://www.tryexponent.com/blog/google-coding-interview-rubric)  
110. Google Software Engineer Interview (questions, process,prep) \- IGotAnOffer, accessed July 29, 2025, [https://igotanoffer.com/blogs/tech/google-software-engineer-interview](https://igotanoffer.com/blogs/tech/google-software-engineer-interview)  
111. Googleyness preparation: Building qualities Google hires for \- Educative.io, accessed July 29, 2025, [https://www.educative.io/blog/googleyness-preparation](https://www.educative.io/blog/googleyness-preparation)  
112. How to Clear Googleyness Round in Google Interview \- Preplaced, accessed July 29, 2025, [https://www.preplaced.in/blog/how-to-clear-googleyness-round-in-google-interview](https://www.preplaced.in/blog/how-to-clear-googleyness-round-in-google-interview)  
113. Googleyness & Leadership Interview Questions (+ how to impress) \- IGotAnOffer, accessed July 29, 2025, [https://igotanoffer.com/blogs/tech/googleyness-leadership-interview-questions](https://igotanoffer.com/blogs/tech/googleyness-leadership-interview-questions)  
114. Show "Googleyness" in Google Software Engineer Interview, accessed July 29, 2025, [https://interviewkickstart.com/blogs/interview-questions/google-software-engineer-interview-sample-questions-and-tips](https://interviewkickstart.com/blogs/interview-questions/google-software-engineer-interview-sample-questions-and-tips)  
115. Code Review Guidelines contribute \- GitLab Docs, accessed July 29, 2025, [https://docs.gitlab.com/development/code\_review/](https://docs.gitlab.com/development/code_review/)  
116. Google Engineering Practices Documentation | eng-practices, accessed July 29, 2025, [https://google.github.io/eng-practices/](https://google.github.io/eng-practices/)  
117. Code Review Developer Guide by Google \- Slab Library, accessed July 29, 2025, [https://slab.com/library/templates/google-code-review/](https://slab.com/library/templates/google-code-review/)  
118. Code review process | Blockly \- Google for Developers, accessed July 29, 2025, [https://developers.google.com/blockly/guides/contribute/get-started/pr\_review\_process](https://developers.google.com/blockly/guides/contribute/get-started/pr_review_process)  
119. accessed December 31, 1969, [https.google.github.io/eng-practices/review/reviewer/standard.html](http://docs.google.com/https.google.github.io/eng-practices/review/reviewer/standard.html)  
120. NLP Dataset for Text Analysis​ | Kaggle, accessed July 29, 2025, [https://www.kaggle.com/datasets/yashdogra/nlpdataset](https://www.kaggle.com/datasets/yashdogra/nlpdataset)  
121. NLP Datasets of Text, Image and Audio \- GeeksforGeeks, accessed July 29, 2025, [https://www.geeksforgeeks.org/nlp/best-nlp-datasets/](https://www.geeksforgeeks.org/nlp/best-nlp-datasets/)  
122. Best 25 Datasets for NLP Projects \- Kaggle, accessed July 29, 2025, [https://www.kaggle.com/discussions/general/150720](https://www.kaggle.com/discussions/general/150720)  
123. 25 Best NLP Datasets for Machine Learning \- iMerit, accessed July 29, 2025, [https://imerit.net/resources/blog/25-best-nlp-datasets-for-machine-learning-all-pbm/](https://imerit.net/resources/blog/25-best-nlp-datasets-for-machine-learning-all-pbm/)  
124. Examples of Data Sets for Text Analysis and NLP Projects, accessed July 29, 2025, [https://ics.uci.edu/\~smyth/courses/cs175/text\_data\_sets.html](https://ics.uci.edu/~smyth/courses/cs175/text_data_sets.html)  
125. Predicting Hiring Decisions in Recruitment Data \- Kaggle, accessed July 29, 2025, [https://www.kaggle.com/datasets/rabieelkharoua/predicting-hiring-decisions-in-recruitment-data](https://www.kaggle.com/datasets/rabieelkharoua/predicting-hiring-decisions-in-recruitment-data)  
126. Interview Success Prediction | Kaggle, accessed July 29, 2025, [https://www.kaggle.com/code/gcdatkin/interview-success-prediction](https://www.kaggle.com/code/gcdatkin/interview-success-prediction)  
127. Automated Prediction and Analysis of Job Interview Performance: The Role of What You Say and How You Say It \- Computer Science : University of Rochester, accessed July 29, 2025, [https://www.cs.rochester.edu/u/gildea/pubs/naim-fg15.pdf](https://www.cs.rochester.edu/u/gildea/pubs/naim-fg15.pdf)  
128. Automated Prediction of Job Interview Performances \- ROC HCI, accessed July 29, 2025, [https://roc-hci.com/past-projects/automated-prediction-of-job-interview-performances/](https://roc-hci.com/past-projects/automated-prediction-of-job-interview-performances/)  
129. Synthetic Data Generation: A Hands-On Guide in Python \- DataCamp, accessed July 29, 2025, [https://www.datacamp.com/tutorial/synthetic-data-generation](https://www.datacamp.com/tutorial/synthetic-data-generation)  
130. Synthetic Data Generation: Tools, Techniques, and Best Practices \- Netscribes, accessed July 29, 2025, [https://www.netscribes.com/expert-speak/getting-started-with-synthetic-data-generation-tools-techniques-and-best-practices](https://www.netscribes.com/expert-speak/getting-started-with-synthetic-data-generation-tools-techniques-and-best-practices)  
131. Synthetic Data Generation Using Large Language Models: Advances in Text and Code, accessed July 29, 2025, [https://arxiv.org/html/2503.14023v1](https://arxiv.org/html/2503.14023v1)  
132. Free interview questions generator \- Workable, accessed July 29, 2025, [https://www.workable.com/interview-questions-generator](https://www.workable.com/interview-questions-generator)